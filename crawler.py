#!/usr/bin/python

import config
import db
import json
import misc
import models
import net
import progressbar
import re

from os.path import basename, splitext, isfile

def get_nist_feed():
    if misc.should_update_nist():
        misc.update_nist()
    else:
        print "Nist feeds are up to date"
    years = misc.get_nist_feed_years()
    cves = []
    url_set = set() # Urls might not be unique.
    cves_to_urls = []
    for year in years:
        path = misc.json_path_for_year(year)
        print "Processing {0}".format(path)
        with open(path, "r") as f:
            json_obj = json.load(f)
            cve_items = json_obj["CVE_Items"]
            bar = progressbar.ProgressBar(maxval=len(cve_items))
            for cve_item in bar(cve_items):
                # Get data from JSON.
                cve_string = cve_item["cve"]["CVE_data_meta"]["ID"]
                urls = [x["url"] for x in cve_item["cve"]["references"]["reference_data"]]

                # Store data in tmp containers.
                cves.append((("cve_string", cve_string), ))
                url_set.update(urls)
                for url in urls:
                    cves_to_urls.append({"url": url, "cve": cve_string})
        print "Done parsing {0}".format(path)
    print "Done parsing files. Storing results into the DB"
    urls = [(("url", url_string), ) for url_string in url_set]

    cve_objects = db.find_or_add_objects(models.CVE, cves, need_ids=True)
    url_objects = db.find_or_add_objects(models.URL, urls, need_ids=True)

    cves_to_urls_ids = []
    print "Matching urls and cves"
    bar = progressbar.ProgressBar(maxval=len(cves_to_urls))
    for x in bar(cves_to_urls):
        # Find corresponding cve and url.
        cve = cve_objects[(("cve_string", x["cve"]),)]
        url = url_objects[(("url", x["url"]),)]
        # Craft a table record using the cve and url ids.
        cves_to_urls_ids.append((("cve_id", cve.id), ("url_id", url.id)))

    db.find_or_add_for_mtm_table(models.cve_url_table, cves_to_urls_ids)

def crawl_android_cve_checker():
    api_url_t = "https://api.github.com/repos/torvalds/linux/commits/{0}"
    http_url_t = "https://github.com/torvalds/linux/commit/{0}"
    crawl_android_cve_checker.count = 0
    print "starting crawl_android_cve_checker()"
    def file_func(file_obj):
        name, ext = splitext(basename(file_obj["path"]))
        if ext != ".patch":
            return []
        cve = db.find_object_orm(models.CVE, cve_string=name)
        if cve is None:
            return []
        raw = net.get_raw_resource(file_obj["download_url"], net.github_auth)
        hashes = re.findall(r"([a-f0-9]{40})", raw)
        for h in hashes:
            api_url = api_url_t.format(h)
            http_url = http_url_t.format(h)
            try:
                net.get_json_resource(api_url, net.github_auth)
            except:
                # not upstream commit. Shit happens
                # print "bad response for resource: " + api_url
                pass
            else:
                # Unoptimized, slow db access methods here, because this
                # repo is supposed to be crawled only once.
                url = db.find_or_add_object_orm(models.URL, url=http_url)
                if not url in cve.working_urls:
                    cve.working_urls.append(url)
                    cve.add_tag("C")
                    cve.add_tag("Linux")
                    cve.add_tag("github_commit")
                    db.session.commit()
                    crawl_android_cve_checker.count += 1
                    print "matched cve: {0} with url: {1}".format(
                            name, http_url)
                    break
        return []

    def dir_func(dir_obj):
        if  "patches" in dir_obj["path"]:
            return True
        return False

    net.crawl_github_repo(user="raymanfx", repo="android-cve-checker",
            file_callback=file_func, dir_callback=dir_func)
    print "crawl_android_cve_checker() finished. Added: {0} CVEs".format(
            crawl_android_cve_checker.count)
    print "=" * 80

def crawl_linux_kernel_cves():
    url = "https://raw.githubusercontent.com/nluedtke/linux_kernel_cves/master/stream_fixes.json"
    http_url_t = "https://github.com/torvalds/linux/commit/{0}"
    patches = net.get_json_resource(url, None)
    bar = progressbar.ProgressBar(maxval=len(patches))
    count = 0
    print "starting crawl_linux_kernel_cves()"
    for k, v in bar(patches.iteritems()):
        # Unoptimized, slow db access methods here, because this
        # repo is supposed to be crawled only once.
        cve = db.find_object_orm(models.CVE, cve_string=k)
        if not cve:
            print "CVE not found?!: {0}".format(k)
            continue
        h = v.values()[0]["cmt_id"]
        url = db.find_or_add_object_orm(models.URL, url=http_url_t.format(h))
        if not url in cve.working_urls:
            cve.working_urls.append(url)
            cve.add_tag("C")
            cve.add_tag("Linux")
            cve.add_tag("github_commit")
            db.session.commit()
            count += 1
    print "crawl_linux_kernel_cves() finished. Added: {0} CVEs".format(count)
    print "=" * 80

# TODO:
# 1. commits
# 2. issues
# 3. pull requests
# 4. add links for all of them if available?

def crawl_nist_feed_for_github():
    print "starting crawl_nist_feed_for_github()"
    urls = db.session.query(models.URL).filter(models.URL.url.like("%github%")).all()
    bar = progressbar.ProgressBar(maxval=len(urls))
    commit_re = re.compile("^https?://github.com/(\S)+/commit/(\S)+")
    issue_re = re.compile("^https?://github.com/(\S)+/issues/(\S)+")
    counter = {"github_commit": 0, "github_issue": 0}
    for url in bar(urls):
        tag = ""
        if re.match(commit_re, url.url):
            tag = "github_commit"
        elif re.match(issue_re, url.url):
            tag = "github_issue"
        else:
            continue
        for cve in url.cves:
            if not url in cve.working_urls:
                cve.add_tag(tag)
                cve.working_urls.append(url)
                counter[tag] += 1

    db.session.commit()
    print "crawl_nist_feed_for_github() done:"
    print "\tgithub commits added: {0}".format(counter["github_commit"])
    print "\tgithub issues added: {0}".format(counter["github_issue"])
    print "=" * 80





