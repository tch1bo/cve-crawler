#!/usr/bin/python

import config
import db
import fs
import json
import itertools
import misc
import models
import net
import os
import re
import stats

from os.path import basename, splitext
from threading import Thread, Event
from Queue import Queue


urls_re_whitelist = [
        re.compile(r"(https?://github.com\S+)"),
        re.compile(r"(https?://svn.apache.org\S+)"),
        re.compile(r"(https?://git.kernel.org\S+)"),
]

def _should_keep_url(url):
    for url_re in urls_re_whitelist:
        if re.match(url_re, url):
            return True
    return False

@stats.record_stats_decorator
def crawl_nist_feed(parallel=False):
    # Works faster without parallelism.
    def items_handler(cve_items):
        get_cve = lambda item: item["cve"]["CVE_data_meta"]["ID"]
        get_urls = lambda item: item["cve"]["references"]["reference_data"]

        queries = \
        [db.InsertQuery(models.CVE, cve_string=get_cve(item))
                for item in cve_items
        ] + \
        [db.InsertQuery(models.URL, url=x["url"])
                for item in cve_items
                for x in get_urls(item)
                if _should_keep_url(x["url"])
        ] + \
        [db.ConnectQuery(models.cve_url_table, get_cve(item), x["url"])
                for item in cve_items for x in get_urls(item)
                if  _should_keep_url(x["url"])
        ]
        return queries

    result = misc.crawl_nist_files(items_handler, parallel=parallel)
    queries = list(itertools.chain(*result))

    print "Done parsing files. Storing results into the DB"

    db.process_queries(queries)

@stats.record_stats_decorator
def crawl_android_cve_checker(local):
    # TODO: add a non-parallel version.
    api_url_t = "https://api.github.com/repos/torvalds/linux/commits/{0}"
    http_url_t = "https://github.com/torvalds/linux/commit/{0}"
    hash_re = re.compile(r"([a-f0-9]{40})")

    counter = stats.Counter(parallel=True)
    bar = misc.UnknownLengthBar(parallel=True)
    def file_func(file_obj):
        name, ext = splitext(basename(file_obj["path"]))
        if ext != ".patch":
            return []
        with db.DBWrapper(parallel=True) as session:
            cve = db.find_object_orm(models.CVE, session=session, cve_string=name)
            if cve is None:
                bar.update()
                return []
            if local:
                raw = fs.read_file(file_obj["path"])
            else:
                raw = net.get_raw_resource(file_obj["download_url"],
                        auth=net.github_auth)
            hashes = re.findall(hash_re, raw)
            for h in hashes:
                api_url = api_url_t.format(h)
                http_url = http_url_t.format(h)
                try:
                    net.get_json_resource(api_url, auth=net.github_auth)
                except:
                    # not upstream commit. Shit happens
                    # print "bad response for resource: " + api_url
                    pass
                else:
                    url = db.find_or_add_object_orm(models.URL, session=session,
                            url=http_url)
                    counter.add_url_to_cve(cve, url, session=session,
                           tags=["C", "Linux", "github"])
            bar.update()
        return []

    def dir_func(dir_obj):
        if  "patches" in dir_obj["path"]:
            return True
        return False

    crawl_github_repo(user="raymanfx", repo="android-cve-checker",
            file_callback=file_func, dir_callback=dir_func, local=local)
    bar.finish()
    return counter

@stats.record_stats_decorator
def crawl_linux_kernel_cves():
    # Works fast enough without parallelism.
    url = "https://raw.githubusercontent.com/nluedtke/linux_kernel_cves/master/stream_fixes.json"
    http_url_t = "https://github.com/torvalds/linux/commit/{0}"
    patches = net.get_json_resource(url)
    bar = misc.KnownLengthBar(maxval=len(patches), parallel=False)

    cves = db.global_session.query(models.CVE).all()
    cve_dict = {cve.cve_string:cve for cve in cves}

    def worker(item):
        cve_string = item[0]
        cve = cve_dict.get(cve_string)
        queries = []
        if cve:
            url_strings = [http_url_t.format(v["cmt_id"]) for v in item[1].values()]
            queries = \
            [db.InsertQuery(models.URL, url=url_string)
                for url_string in url_strings
            ] + \
            [db.ConnectQuery(models.cve_url_table, cve_string, url_string)
                for url_string in url_strings
            ] + \
            [db.UpdateTagQuery(models.CVE, cve_string, ["C", "Linux", "github"])]
        bar.update()
        return queries

    results = map(worker, patches.items())
    bar.finish()

    queries = list(set(itertools.chain(*results)))
    db.process_queries(queries)

@stats.record_stats_decorator
def crawl_vuln_db(local, parallel):
    commit_re = re.compile("(https://github.com/\S+/commit/[a-f0-9]+)")
    cve_re = re.compile("(CVE-\d+-\d+)")
    counter = stats.Counter(parallel=parallel)
    bar = misc.UnknownLengthBar(parallel=parallel)
    tag_dict = {
            "composer": "PHP",
            "golang"  : "Go",
            "maven"   : "Java",
            "npm"     : "JS",
            "nuget"   : "", #has different languages
            "pip"     : "Python",
            "rubygems": "Ruby",
    }
    def file_func(file_obj):
        name = basename(file_obj["path"])
        if local:
            raw = fs.read_file(file_obj["path"])
        else:
            raw = net.get_raw_resource(file_obj["download_url"], auth=net.github_auth)
        # Add/Find urls corresponding to commits.
        commit_urls = list(set(re.findall(commit_re, raw)))
        if not commit_urls:
            bar.update()
            return []
        with db.DBWrapper(parallel=True) as session:
            urls = [db.find_or_add_object_orm(models.URL, session=session,
                url=x) for x in commit_urls]
            # Do the same for CVEs.
            cve_strings = list(set(re.findall(cve_re, raw)))
            if not cve_strings:
                hash_id = models.NonCVE.hash_id_for_urls(commit_urls)
                cves = [db.find_or_add_object_orm(models.NonCVE, session=session,
                    hash_id=hash_id)]
            else:
                cves = [db.find_or_add_object_orm(models.CVE, session=session,
                    cve_string=x) for x in cve_strings]
            # Figure out the tag list.
            tags = ["vulndb"]
            for k, v in tag_dict.iteritems():
                if k in file_obj["path"]:
                    tags.append(v)
                    break
            # Match CVEs and urls.
            for url in urls:
                for cve in cves:
                    counter.add_url_to_cve(cve, url, tags, session=session)
        bar.update()
        return []

    def dir_func(dir_obj):
        if "data" in dir_obj["path"]:
            return True
        return False

    crawl_github_repo(user="snyk", repo="vulnerabilitydb",
            file_callback=file_func, dir_callback=dir_func, local=local)
    bar.finish()

    return counter

@stats.record_stats_decorator
def crawl_django():
    cve_re = re.compile(r":cve:`(\S+)`")
    commit_re = re.compile(r"(https://github.com/\S+/[a-f0-9]+)")
    counter = stats.Counter(parallel=False)

    raw = net.get_raw_resource(
        "https://raw.githubusercontent.com/django/django/master/docs/releases/security.txt")
    indices = [x.start() for x in re.finditer(r":cve:", raw)]
    bar = misc.KnownLengthBar(maxval=len(indices), parallel=False)
    with db.DBWrapper(parallel=False) as session:
        for start, finish in bar(zip(indices, indices[1:] + [len(raw)])):
            sub_string = raw[start:finish]

            # Find the CVE
            cve_string = "CVE-" + re.findall(cve_re, sub_string)[0]
            cve = db.find_object_orm(models.CVE, session=session,
                    cve_string=cve_string)
            if not cve:
                print "CVE not found?!: " + cve_string
                continue

            # Find the URLs
            url_strings = re.findall(commit_re, sub_string)
            if not url_strings:
                continue
            urls = [db.find_or_add_object_orm(models.URL, session=session, url=x)
                    for x in url_strings]

            # Connect cve<->urls
            for url in urls:
                counter.add_url_to_cve(cve, url, ["Python", "Django"],
                        session=session)
        session.commit()

    return counter

@stats.record_stats_decorator
def crawl_debian_security(local, parallel):
    cve_re = re.compile("^(CVE\S+)")
    list_url = "https://salsa.debian.org/security-tracker-team/security-tracker/raw/master/data/CVE/list"

    if not local:
        raw = net.get_raw_resource(list_url)
    else:
        path = misc.repo_path("security-tracker-team", "security-tracker")
        path = os.path.join(path, "raw", "master", "data", "CVE", "list")
        with open(path, "r") as f:
            raw = f.read()

    indices = [x.start() for x in re.finditer(r"^CVE", raw, re.MULTILINE)]
    sub_strings = [raw[s:f] for s,f in zip(indices, indices[1:] + [len(raw)])]
    bar = misc.KnownLengthBar(maxval=len(sub_strings), parallel=parallel)
    with db.DBWrapper(parallel=False) as session:
        cves = session.query(models.CVE).all()
    cve_dict = {cve.cve_string: cve for cve in cves}

    def worker(sub_string):
        cve_string = re.findall(cve_re, sub_string)[0]
        cve = cve_dict.get(cve_string)
        url_strings = []
        if cve:
            for url_re in urls_re_whitelist:
                url_strings += re.findall(url_re, sub_string)
        bar.update()
        return (cve, url_strings)

    # result = filter(lambda x: x[0] and x[1], misc.parallel_map(worker, sub_strings))
    result = filter(lambda x: x[0] and x[1], misc.map_func(worker, sub_strings, parallel))

    with db.DBWrapper(parallel=False) as session:
        url_strings_set = set()
        for r in result:
            url_strings_set.update(r[1])

        url_queries = list([(("url", x), ) for x in url_strings_set])
        urls = db.find_or_add_objects(models.URL, url_queries,
                session=session, need_ids=True)
        cve_url_ids = []
        for r in result:
            cve_id = r[0].id
            url_ids = [urls[(("url", x), )].id for x in r[1]]
            cve_url_ids += [(("cve_id", cve_id), ("url_id", url_id)) for url_id in url_ids]

    counter = stats.Counter(parallel=False)
    for r in result:
        if not r[0].urls:
            counter.unique_cves_count += 1
    _, counter.working_urls_count = \
        db.find_or_add_for_mtm_table(models.cve_url_table, cve_url_ids)
    return counter

@stats.record_stats_decorator
def crawl_debian_fake_names(parallel):
    main_page = net.get_raw_resource("https://security-tracker.debian.org/tracker/data/fake-names")
    temp_re = re.compile(r"/tracker/(TEMP-[0-9A-F-]+)")
    href_re = re.compile(r'href="(\S+?)"')
    counter = stats.Counter(parallel=parallel)

    temps = re.findall(temp_re, main_page)
    bar = misc.KnownLengthBar(maxval=len(temps), parallel=parallel)
    def worker(temp):
        bar.update()
        raw = net.get_raw_resource("https://security-tracker.debian.org/tracker/{0}".format(temp))
        hrefs = re.findall(href_re, raw)
        url_strings = filter(_should_keep_url, hrefs)
        if not url_strings:
            return
        with db.DBWrapper(parallel=parallel) as session:
            noncve = db.find_or_add_object_orm(models.NonCVE, session=session,
                    hash_id=temp)
            for url_string in url_strings:
                url = db.find_or_add_object_orm(models.URL, session=session,
                        url=url_string)
                counter.add_url_to_cve(noncve, url, ["DebianFake"], session=session)
    misc.map_func(worker, temps, parallel=parallel)
    return counter

def crawl_github_repo(user, repo, file_callback, dir_callback, local):
    # TODO: add a non-parallel version.
    queue = Queue()
    if local:
        path = misc.repo_path(user, repo)
        crawl_list = fs.crawl_dir(path)
    else:
        base_url = net.get_api_url(user, repo)
        crawl_list = net.get_json_resource(base_url, auth=net.github_auth)

    def worker(queue):
        while True:
            item = queue.get()
            crawl_list = []
            if item["type"] == "file":
                crawl_list = file_callback(item)
            elif dir_callback(item):
                if local:
                    crawl_list = fs.crawl_dir(item["path"])
                else:
                    crawl_list = net.get_json_resource(item["url"], auth=net.github_auth)
            for item in crawl_list: queue.put(item)
            queue.task_done()

    for item in crawl_list: queue.put(item)
    for i in xrange(0, config.THREADS_COUNT):
        t = Thread(target=worker, args=(queue, ))
        # TODO: can't make the threads stop properly when an event is fired.
        # So for now we'll do it with daemon threads, which will get blocked
        # after the queue gets empty.
        t.daemon = True
        t.start()
    queue.join()

