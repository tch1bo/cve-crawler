#!/usr/bin/python

import config
import csv
import db
import models
import net
import progressbar
import re

from os.path import basename, splitext

def store_initial_cves():
    url_re = re.compile(r'\s+(CONFIRM|URL|MISC):(\S+)')
    print "Parsing file: {0}".format(config.CVE_CSV_FILE)
    num_lines = sum(1 for line in open(config.CVE_CSV_FILE))
    bar = progressbar.ProgressBar(maxval=num_lines)
    cves = []
    url_set = set() # Urls might not be unique.
    cves_to_urls = []

    with open(config.CVE_CSV_FILE, "r") as f:
        csv_reader = csv.reader(f)
        for cve_record in bar(csv_reader):
            cves.append((("cve_string", cve_record[0]), ))
            for field in cve_record:
                for tmp in re.findall(url_re, field):
                    url_string = tmp[1]
                    if not url_string in url_set:
                        url_set.add(url_string)
                    cves_to_urls.append({"url": url_string, "cve": cve_record[0]})

        urls = [(("url", url_string), ) for url_string in url_set]

        cve_objects = db.find_or_add_objects(models.CVE, cves, need_ids=True)
        url_objects = db.find_or_add_objects(models.URL, urls, need_ids=True)

        cves_to_urls_ids = []
        print "Matching urls and cves"
        bar = progressbar.ProgressBar(maxval=len(cves_to_urls))
        for x in bar(cves_to_urls):
            # Find corresponding cve and url.
            cve = cve_objects[(("cve_string", x["cve"]),)]
            url = url_objects[(("url", x["url"]),)]
            # Craft a table record using the cve and url ids.
            cves_to_urls_ids.append((("cve_id", cve.id), ("url_id", url.id)))

        db.find_or_add_for_mtm_table(models.cve_url_table, cves_to_urls_ids)

# Too many commits are not found using only the commit hash
# (only 104/297 are found).
# TODO: also search using the commit message (remove the word [PATCH]).
# Also it might not make sense, because the repo
# https://github.com/nluedtke/linux_kernel_cves has better info.
def crawl_android_cve_checker():
    api_url_t = "https://api.github.com/repos/torvalds/linux/commits/{0}"
    http_url_t = "https://github.com/torvalds/linux/commit/{0}"
    crawl_android_cve_checker.count = 0
    print "starting crawl_android_cve_checker()"
    def file_func(file_obj):
        name, ext = splitext(basename(file_obj["path"]))
        if ext != ".patch":
            return []
        cve = db.find_object_orm(models.CVE, cve_string=name)
        if cve is None:
            return []
        raw = net.get_raw_resource(file_obj["download_url"], net.github_auth)
        hashes = re.findall(r"([a-f0-9]{40})", raw)
        for h in hashes:
            api_url = api_url_t.format(h)
            http_url = http_url_t.format(h)
            try:
                net.get_json_resource(api_url, net.github_auth)
            except:
                # not upstream commit. Shit happens
                # print "bad response for resource: " + api_url
                pass
            else:
                # Unoptimized, slow db access methods here, because this
                # repo is supposed to be crawled only once.
                url = db.find_or_add_object_orm(models.URL, url=http_url)
                if not url in cve.working_urls:
                    cve.working_urls.append(url)
                    cve.add_tag("C")
                    cve.add_tag("Linux")
                    db.session.commit()
                    crawl_android_cve_checker.count += 1
                    print "matched cve: {0} with url: {1}".format(
                            name, http_url)
                    break
        return []

    def dir_func(dir_obj):
        if  "patches" in dir_obj["path"]:
            return True
        return False

    net.crawl_github_repo(user="raymanfx", repo="android-cve-checker",
            file_callback=file_func, dir_callback=dir_func)
    print "crawl_android_cve_checker() finished. Added: {0} CVEs".format(
            crawl_android_cve_checker.count)
    print "=" * 80

def crawl_linux_kernel_cves():
    url = "https://raw.githubusercontent.com/nluedtke/linux_kernel_cves/master/stream_fixes.json"
    http_url_t = "https://github.com/torvalds/linux/commit/{0}"
    patches = net.get_json_resource(url, None)
    bar = progressbar.ProgressBar(maxval=len(patches))
    count = 0
    print "starting crawl_linux_kernel_cves()"
    for k, v in bar(patches.iteritems()):
        # Unoptimized, slow db access methods here, because this
        # repo is supposed to be crawled only once.
        cve = db.find_object_orm(models.CVE, cve_string=k)
        if not cve:
            print "CVE not found?!: {0}".format(k)
            continue
        h = v.values()[0]["cmt_id"]
        url = db.find_or_add_object_orm(models.URL, url=http_url_t.format(h))
        if not url in cve.working_urls:
            cve.working_urls.append(url)
            cve.add_tag("C")
            cve.add_tag("Linux")
            db.session.commit()
            count += 1
    print "crawl_linux_kernel_cves() finished. Added: {0} CVEs".format(count)
    print "=" * 80
