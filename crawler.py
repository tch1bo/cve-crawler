#!/usr/bin/python

import config
import db
import fs
import json
import misc
import models
import net
import os
import progressbar
import re
import stats

from os.path import basename, splitext
from threading import Thread, Event
from Queue import Queue


urls_re_whitelist = [
        re.compile(r"(https?://github.com\S+)"),
        re.compile(r"(https?://svn.apache.org\S+)"),
        re.compile(r"(https?://git.kernel.org\S+)"),
]

def _should_keep_url(url):
    for url_re in urls_re_whitelist:
        if re.match(url_re, url):
            return True
    return False

@stats.record_stats_decorator
def crawl_nist_feed(parallel):
    # We're not accessing the counter in a parallel way in this function.
    counter = stats.Counter(parallel=False)

    def items_handler(cve_items):
        get_cve = lambda item: item["cve"]["CVE_data_meta"]["ID"]
        get_urls = lambda item: item["cve"]["references"]["reference_data"]

        cves = [(("cve_string", get_cve(item)), ) for item in cve_items]
        urls = [x["url"] for item in cve_items for x in get_urls(item)
                if _should_keep_url(x["url"])]
        cves_to_urls = [{"url": x["url"], "cve": get_cve(item)}
                for item in cve_items for x in get_urls(item)
                if  _should_keep_url(x["url"])]
        return [cves, urls, cves_to_urls]

    # This works faster in single-threaded mode.
    result = misc.crawl_nist_files(items_handler, parallel=parallel)
    join_result = lambda index: [x for launch in result for x in launch[index]]

    cves = join_result(0)
    urls = [(("url", url_string), ) for url_string in set(join_result(1))]
    cves_to_urls = join_result(2)

    print "Done parsing files. Storing results into the DB"

    cve_objects = db.find_or_add_objects(models.CVE, cves,
            session=db.global_session, need_ids=True)
    url_objects = db.find_or_add_objects(models.URL, urls,
            session=db.global_session, need_ids=True)

    cves_to_urls_ids = []
    print "Matching urls and cves"
    bar = progressbar.ProgressBar(maxval=len(cves_to_urls))
    for x in bar(cves_to_urls):
        # Find corresponding cve and url.
        cve = cve_objects[(("cve_string", x["cve"]),)]
        url = url_objects[(("url", x["url"]),)]
        # Craft a table record using the cve and url ids.
        cves_to_urls_ids.append((("cve_id", cve.id), ("url_id", url.id)))

    counter.unique_cves_count, counter.working_urls_count = \
        db.find_or_add_for_mtm_table(models.cve_url_table, cves_to_urls_ids)
    return counter

@stats.record_stats_decorator
def crawl_android_cve_checker(local):
    # TODO: add a non-parallel version.
    api_url_t = "https://api.github.com/repos/torvalds/linux/commits/{0}"
    http_url_t = "https://github.com/torvalds/linux/commit/{0}"
    hash_re = re.compile(r"([a-f0-9]{40})")

    outter = crawl_android_cve_checker
    outter.counter = stats.Counter(parallel=True)
    outter.bar = misc.UnknownLengthBar(parallel=True)
    def file_func(file_obj):
        outter = crawl_android_cve_checker
        outter.bar.update()
        name, ext = splitext(basename(file_obj["path"]))
        if ext != ".patch":
            return []
        with db.DBWrapper(parallel=True) as session:
            cve = db.find_object_orm(models.CVE, session=session, cve_string=name)
            if cve is None:
                return []
            if local:
                raw = fs.read_file(file_obj["path"])
            else:
                raw = net.get_raw_resource(file_obj["download_url"],
                        auth=net.github_auth)
            hashes = re.findall(hash_re, raw)
            for h in hashes:
                api_url = api_url_t.format(h)
                http_url = http_url_t.format(h)
                try:
                    net.get_json_resource(api_url, auth=net.github_auth)
                except:
                    # not upstream commit. Shit happens
                    # print "bad response for resource: " + api_url
                    pass
                else:
                    url = db.find_or_add_object_orm(models.URL, session=session,
                            url=http_url)
                    outter.counter.add_url_to_cve(cve, url, session=session,
                           tags=["C", "Linux", "github"])
        return []

    def dir_func(dir_obj):
        if  "patches" in dir_obj["path"]:
            return True
        return False

    crawl_github_repo(user="raymanfx", repo="android-cve-checker",
            file_callback=file_func, dir_callback=dir_func, local=local)
    outter.bar.finish()
    return outter.counter

@stats.record_stats_decorator
def crawl_linux_kernel_cves():
    url = "https://raw.githubusercontent.com/nluedtke/linux_kernel_cves/master/stream_fixes.json"
    http_url_t = "https://github.com/torvalds/linux/commit/{0}"
    patches = net.get_json_resource(url)
    bar = progressbar.ProgressBar(maxval=len(patches))
    counter = stats.Counter()
    for k, v in bar(patches.iteritems()):
        cve = db.find_object_orm(models.CVE, cve_string=k)
        if not cve:
            # print "CVE not found?!: {0}".format(k)
            continue
        h = v.values()[0]["cmt_id"]
        url = db.find_or_add_object_orm(models.URL, url=http_url_t.format(h))
        counter.add_url_to_cve(cve, url, ["C", "Linux", "github"])
    db.session.commit()
    return counter

@stats.record_stats_decorator
def crawl_vuln_db(local):
    commit_re = re.compile("(https://github.com/\S+/commit/[a-f0-9]+)")
    cve_re = re.compile("(CVE-\d+-\d+)")
    crawl_vuln_db.counter = stats.Counter()
    crawl_vuln_db.bar = misc.UnknownLengthBar()
    tag_dict = {
            "composer": "PHP",
            "golang"  : "Go",
            "maven"   : "Java",
            "npm"     : "JS",
            "nuget"   : "", #has different languages
            "pip"     : "Python",
            "rubygems": "Ruby",
    }
    def file_func(file_obj):
        crawl_vuln_db.bar.update()
        name = basename(file_obj["path"])
        if name != "README.md":
            pass
        if local:
            raw = fs.read_file(file_obj["path"])
        else:
            raw = net.get_raw_resource(file_obj["download_url"], auth=net.github_auth)
        # Add/Find urls correpsonding to commits.
        commit_urls = list(set(re.findall(commit_re, raw)))
        if not commit_urls:
            return []
        urls = [db.find_or_add_object_orm(models.URL, url=x) for x in commit_urls]
        # Do the same for CVEs.
        cve_strings = list(set(re.findall(cve_re, raw)))
        if not cve_strings:
            hash_id = models.NonCVE.hash_id_for_urls(commit_urls)
            cves = [db.find_or_add_object_orm(models.NonCVE, hash_id=hash_id)]
        else:
            cves = [db.find_or_add_object_orm(models.CVE, cve_string=x) for x in cve_strings]
        # Figure out the tag list.
        tags = ["vulndb"]
        for k, v in tag_dict.iteritems():
            if k in file_obj["path"]:
                tags.append(v)
                break
        # Match CVEs and urls.
        for url in urls:
            for cve in cves:
                crawl_vuln_db.counter.add_url_to_cve(cve, url, tags)
        return []

    def dir_func(dir_obj):
        if "data" in dir_obj["path"]:
            return True
        return False

    crawl_github_repo(user="snyk", repo="vulnerabilitydb",
            file_callback=file_func, dir_callback=dir_func, local=local)
    crawl_vuln_db.bar.finish()
    db.session.commit()

    return crawl_vuln_db.counter

@stats.record_stats_decorator
def crawl_django():
    cve_re = re.compile(r":cve:`(\S+)`")
    commit_re = re.compile(r"(https://github.com/\S+/[a-f0-9]+)")
    counter = stats.Counter()

    raw = net.get_raw_resource(
        "https://raw.githubusercontent.com/django/django/master/docs/releases/security.txt")
    indices = [x.start() for x in re.finditer(r":cve:", raw)]
    bar = progressbar.ProgressBar(maxval=len(indices))
    for start, finish in bar(zip(indices, indices[1:] + [len(raw)])):
        sub_string = raw[start:finish]

        # Find the CVE
        cve_string = "CVE-" + re.findall(cve_re, sub_string)[0]
        cve = db.find_object_orm(models.CVE, cve_string=cve_string)
        if not cve:
            print "CVE not found?!: " + cve_string
            continue

        # Find the URLs
        url_strings = re.findall(commit_re, sub_string)
        if not url_strings:
            continue
        urls = [db.find_or_add_object_orm(models.URL, url=x) for x in url_strings]

        # Connect cve<->urls
        for url in urls:
            counter.add_url_to_cve(cve, url, ["Python", "Django"])
    db.session.commit()

    return counter

@stats.record_stats_decorator
def crawl_debian_security(local):
    cve_re = re.compile("^(CVE\S+)")
    counter = stats.Counter()
    list_url = "https://salsa.debian.org/security-tracker-team/security-tracker/raw/master/data/CVE/list"

    if not local:
        raw = net.get_raw_resource(list_url)
    else:
        path = misc.repo_path("security-tracker-team", "security-tracker")
        path = os.path.join(path, "raw", "master", "data", "CVE", "list")
        with open(path, "r") as f:
            raw = f.read()

    indices = [x.start() for x in re.finditer(r"^CVE", raw, re.MULTILINE)]
    bar = progressbar.ProgressBar(maxval=len(indices))
    for start, finish in bar(zip(indices, indices[1:] + [len(raw)])):
        sub_string = raw[start:finish]

        # Find the CVE.
        cve_string = re.findall(cve_re, sub_string)[0]
        cve = db.find_object_orm(models.CVE, cve_string=cve_string)
        if not cve:
            # Reserved cve. Skip it.
            continue

        # Find the URLs.
        url_strings = []
        for url_re in urls_re_whitelist:
            url_strings += re.findall(url_re, sub_string)
        if not url_strings:
            continue
        urls = [db.find_or_add_object_orm(models.URL, url=x) for x in url_strings]

        # Connect cve<->urls
        for url in urls:
            counter.add_url_to_cve(cve, url, ["DebianSec"])
    db.session.commit()

    return counter

@stats.record_stats_decorator
def crawl_debian_fake_names():
    main_page = net.get_raw_resource("https://security-tracker.debian.org/tracker/data/fake-names")
    temp_re = re.compile(r"/tracker/(TEMP-[0-9A-F-]+)")
    href_re = re.compile(r'href="(\S+?)"')
    counter = stats.Counter()

    temps = re.findall(temp_re, main_page)
    bar = progressbar.ProgressBar(maxval=len(temps))
    for temp in bar(temps):
        #TODO: add multithreading here?
        raw = net.get_raw_resource("https://security-tracker.debian.org/tracker/{0}".format(temp))
        hrefs = re.findall(href_re, raw)
        url_strings = filter(_should_keep_url, hrefs)
        if not url_strings:
            continue
        noncve = db.find_or_add_object_orm(models.NonCVE, hash_id=temp)
        for url_string in url_strings:
            url = db.find_or_add_object_orm(models.URL, url=url_string)
            counter.add_url_to_cve(noncve, url, ["DebianFake"])
    db.session.commit()
    return counter

def crawl_github_repo(user, repo, file_callback, dir_callback, local):
    # TODO: add a non-parallel version.
    queue = Queue()
    if local:
        path = misc.repo_path(user, repo)
        crawl_list = fs.crawl_dir(path)
    else:
        base_url = net.get_api_url(user, repo)
        crawl_list = net.get_json_resource(base_url, net.github_auth)

    def worker(queue):
        while True:
            item = queue.get()
            crawl_list = []
            if item["type"] == "file":
                crawl_list = file_callback(item)
            elif dir_callback(item):
                if local:
                    crawl_list = fs.crawl_dir(item["path"])
                else:
                    crawl_list = net.get_json_resource(item["url"], net.github_auth)
            for item in crawl_list: queue.put(item)
            queue.task_done()

    for item in crawl_list: queue.put(item)
    for i in xrange(0, config.THREADS_COUNT):
        t = Thread(target=worker, args=(queue, ))
        # TODO: can't make the threads stop properly when an event is fired.
        # So for now we'll do it with daemon threads, which will get blocked
        # after the queue gets empty.
        t.daemon = True
        t.start()
    queue.join()

