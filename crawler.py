#!/usr/bin/python

import config
import db
import fs
import json
import misc
import models
import net
import os
import progressbar
import re
import stats

from os.path import basename, splitext

class Counter:
    def __init__(self):
        self.unique_cves_count = 0
        self.working_urls_count = 0

    def add_url_to_cve(self, cve, url, tags):
        if url in cve.urls:
            return
        if not cve.urls:
            self.unique_cves_count += 1
        for tag in tags:
            cve.add_tag(tag)
        cve.urls.append(url)
        self.working_urls_count += 1

_ok_urls_res = [
        re.compile(r"(https?://github.com\S+)"),
        re.compile(r"(https?://svn.apache.org\S+)"),
        re.compile(r"(https?://git.kernel.org\S+)"),
]
def _should_keep_url(url):
    for url_re in _ok_urls_res:
        if re.match(url_re, url):
            return True
    return False

@stats.record_stats_decorator
def crawl_nist_feed():
    if misc.should_update_nist():
        misc.update_nist()
    else:
        print "Nist feeds are up to date"
    years = misc.get_nist_feed_years()
    cves = []
    url_set = set() # Urls might not be unique.
    cves_to_urls = []
    counter = Counter()
    for year in years:
        path = misc.json_path_for_year(year)
        print "Processing {0}".format(path)
        with open(path, "r") as f:
            json_obj = json.load(f)
            cve_items = json_obj["CVE_Items"]
            bar = progressbar.ProgressBar(maxval=len(cve_items))
            for cve_item in bar(cve_items):
                # Get data from JSON.
                cve_string = cve_item["cve"]["CVE_data_meta"]["ID"]
                urls = [x["url"] for x in cve_item["cve"]["references"]["reference_data"]]
                if not urls:
                    # Don't store the RESERVED CVEs, etc.
                    continue
                urls = filter(_should_keep_url, urls)

                # Store data in tmp containers.
                cves.append((("cve_string", cve_string), ))
                url_set.update(urls)
                for url in urls:
                    cves_to_urls.append({"url": url, "cve": cve_string})
        print "Done parsing {0}".format(path)
    print "Done parsing files. Storing results into the DB"
    urls = [(("url", url_string), ) for url_string in url_set]

    cve_objects = db.find_or_add_objects(models.CVE, cves, need_ids=True)
    url_objects = db.find_or_add_objects(models.URL, urls, need_ids=True)

    cves_to_urls_ids = []
    print "Matching urls and cves"
    bar = progressbar.ProgressBar(maxval=len(cves_to_urls))
    for x in bar(cves_to_urls):
        # Find corresponding cve and url.
        cve = cve_objects[(("cve_string", x["cve"]),)]
        url = url_objects[(("url", x["url"]),)]
        # Craft a table record using the cve and url ids.
        cves_to_urls_ids.append((("cve_id", cve.id), ("url_id", url.id)))

    counter.unique_cves_count, counter.working_urls_count = \
        db.find_or_add_for_mtm_table(models.cve_url_table, cves_to_urls_ids)
    return counter

@stats.record_stats_decorator
def crawl_android_cve_checker(local):
    api_url_t = "https://api.github.com/repos/torvalds/linux/commits/{0}"
    http_url_t = "https://github.com/torvalds/linux/commit/{0}"
    hash_re = re.compile(r"([a-f0-9]{40})")
    # A small dirty hack to access the variables in the inner function.
    outter = crawl_android_cve_checker
    outter.counter = Counter()
    outter.bar = misc.UnknownLengthBar()
    def file_func(file_obj):
        outter = crawl_android_cve_checker
        outter.bar.update()
        name, ext = splitext(basename(file_obj["path"]))
        if ext != ".patch":
            return []
        cve = db.find_object_orm(models.CVE, cve_string=name)
        if cve is None:
            return []
        if local:
            raw = fs.read_file(file_obj["path"])
        else:
            raw = net.get_raw_resource(file_obj["download_url"], net.github_auth)
        hashes = re.findall(hash_re, raw)
        for h in hashes:
            api_url = api_url_t.format(h)
            http_url = http_url_t.format(h)
            try:
                net.get_json_resource(api_url, net.github_auth)
            except:
                # not upstream commit. Shit happens
                # print "bad response for resource: " + api_url
                pass
            else:
                url = db.find_or_add_object_orm(models.URL, url=http_url)
                outter.counter.add_url_to_cve(cve, url,
                        ["C", "Linux", "github"])
        return []

    def dir_func(dir_obj):
        if  "patches" in dir_obj["path"]:
            return True
        return False

    crawl_github_repo(user="raymanfx", repo="android-cve-checker",
            file_callback=file_func, dir_callback=dir_func, local=local)
    outter.bar.finish()
    db.session.commit()

    return outter.counter

@stats.record_stats_decorator
def crawl_linux_kernel_cves():
    url = "https://raw.githubusercontent.com/nluedtke/linux_kernel_cves/master/stream_fixes.json"
    http_url_t = "https://github.com/torvalds/linux/commit/{0}"
    patches = net.get_json_resource(url, None)
    bar = progressbar.ProgressBar(maxval=len(patches))
    counter = Counter()
    for k, v in bar(patches.iteritems()):
        cve = db.find_object_orm(models.CVE, cve_string=k)
        if not cve:
            # print "CVE not found?!: {0}".format(k)
            continue
        h = v.values()[0]["cmt_id"]
        url = db.find_or_add_object_orm(models.URL, url=http_url_t.format(h))
        counter.add_url_to_cve(cve, url, ["C", "Linux", "github"])
    db.session.commit()
    return counter

@stats.record_stats_decorator
def crawl_vuln_db(local):
    commit_re = re.compile("(https://github.com/\S+/commit/[a-f0-9]+)")
    cve_re = re.compile("(CVE-\d+-\d+)")
    crawl_vuln_db.counter = Counter()
    crawl_vuln_db.bar = misc.UnknownLengthBar()
    tag_dict = {
            "composer": "PHP",
            "golang"  : "Go",
            "maven"   : "Java",
            "npm"     : "JS",
            "nuget"   : "", #has different languages
            "pip"     : "Python",
            "rubygems": "Ruby",
    }
    def file_func(file_obj):
        crawl_vuln_db.bar.update()
        name = basename(file_obj["path"])
        if name != "README.md":
            pass
        if local:
            raw = fs.read_file(file_obj["path"])
        else:
            raw = net.get_raw_resource(file_obj["download_url"], net.github_auth)
        # Add/Find urls correpsonding to commits.
        commit_urls = list(set(re.findall(commit_re, raw)))
        if not commit_urls:
            return []
        urls = [db.find_or_add_object_orm(models.URL, url=x) for x in commit_urls]
        # Do the same for CVEs.
        cve_strings = list(set(re.findall(cve_re, raw)))
        if not cve_strings:
            hash_id = models.NonCVE.hash_id_for_urls(commit_urls)
            cves = [db.find_or_add_object_orm(models.NonCVE, hash_id=hash_id)]
        else:
            cves = [db.find_or_add_object_orm(models.CVE, cve_string=x) for x in cve_strings]
        # Figure out the tag list.
        tags = ["vulndb"]
        for k, v in tag_dict.iteritems():
            if k in file_obj["path"]:
                tags.append(v)
                break
        # Match CVEs and urls.
        for url in urls:
            for cve in cves:
                crawl_vuln_db.counter.add_url_to_cve(cve, url, tags)
        return []

    def dir_func(dir_obj):
        if "data" in dir_obj["path"]:
            return True
        return False

    crawl_github_repo(user="snyk", repo="vulnerabilitydb",
            file_callback=file_func, dir_callback=dir_func, local=local)
    crawl_vuln_db.bar.finish()
    db.session.commit()

    return crawl_vuln_db.counter

@stats.record_stats_decorator
def crawl_django():
    cve_re = re.compile(r":cve:`(\S+)`")
    commit_re = re.compile(r"(https://github.com/\S+/[a-f0-9]+)")
    counter = Counter()

    raw = net.get_raw_resource(auth=None, url=
        "https://raw.githubusercontent.com/django/django/master/docs/releases/security.txt")
    indices = [x.start() for x in re.finditer(r":cve:", raw)]
    bar = progressbar.ProgressBar(maxval=len(indices))
    for start, finish in bar(zip(indices, indices[1:] + [len(raw)])):
        sub_string = raw[start:finish]

        # Find the CVE
        cve_string = "CVE-" + re.findall(cve_re, sub_string)[0]
        cve = db.find_object_orm(models.CVE, cve_string=cve_string)
        if not cve:
            print "CVE not found?!: " + cve_string
            continue

        # Find the URLs
        url_strings = re.findall(commit_re, sub_string)
        if not url_strings:
            continue
        urls = [db.find_or_add_object_orm(models.URL, url=x) for x in url_strings]

        # Connect cve<->urls
        for url in urls:
            counter.add_url_to_cve(cve, url, ["Python", "Django"])
    db.session.commit()

    return counter

@stats.record_stats_decorator
def crawl_debian_security(local):
    cve_re = re.compile("^(CVE\S+)")
    counter = Counter()

    if not local:
        raw = net.get_raw_resource(auth=None, url=
        "https://salsa.debian.org/security-tracker-team/security-tracker/raw/master/data/CVE/list")
    else:
        path = misc.repo_path("security-tracker-team", "security-tracker")
        path = os.path.join(path, "raw", "master", "data", "CVE", "list")
        with open(path, "r") as f:
            raw = f.read()

    indices = [x.start() for x in re.finditer(r"^CVE", raw, re.MULTILINE)]
    bar = progressbar.ProgressBar(maxval=len(indices))
    for start, finish in bar(zip(indices, indices[1:] + [len(raw)])):
        sub_string = raw[start:finish]

        # Find the CVE.
        cve_string = re.findall(cve_re, sub_string)[0]
        cve = db.find_object_orm(models.CVE, cve_string=cve_string)
        if not cve:
            # Reserved cve. Skip it.
            continue

        # Find the URLs.
        url_strings = []
        for url_re in _ok_urls_res:
            url_strings += re.findall(url_re, sub_string)
        if not url_strings:
            continue
        urls = [db.find_or_add_object_orm(models.URL, url=x) for x in url_strings]

        # Connect cve<->urls
        for url in urls:
            counter.add_url_to_cve(cve, url, ["DebianSec"])
    db.session.commit()

    return counter

@stats.record_stats_decorator
def crawl_debian_fake_names():
    main_page = net.get_raw_resource(auth=None, url=
            "https://security-tracker.debian.org/tracker/data/fake-names")
    temp_re = re.compile(r"/tracker/(TEMP-[0-9A-F-]+)")
    href_re = re.compile(r'href="(\S+?)"')
    counter = Counter()

    temps = re.findall(temp_re, main_page)
    bar = progressbar.ProgressBar(maxval=len(temps))
    for temp in bar(temps):
        #TODO: add multithreading here?
        raw = net.get_raw_resource(auth=None, url=
                "https://security-tracker.debian.org/tracker/{0}".format(temp))
        hrefs = re.findall(href_re, raw)
        url_strings = filter(_should_keep_url, hrefs)
        if not url_strings:
            continue
        noncve = db.find_or_add_object_orm(models.NonCVE, hash_id=temp)
        for url_string in url_strings:
            url = db.find_or_add_object_orm(models.URL, url=url_string)
            counter.add_url_to_cve(noncve, url, ["DebianFake"])
    db.session.commit()
    return counter

def crawl_github_repo(user, repo, file_callback, dir_callback, local):
    if local:
        path = misc.repo_path(user, repo)
        crawl_list = fs.crawl_dir(path)
    else:
        base_url = net.get_api_url(user, repo)
        crawl_list = net.get_json_resource(base_url, net.github_auth)
    for item in crawl_list:
        if item["type"] == "file":
            crawl_list += file_callback(item)
        elif dir_callback(item):
            if local:
                crawl_list += fs.crawl_dir(item["path"])
            else:
                crawl_list += net.get_json_resource(item["url"], net.github_auth)
