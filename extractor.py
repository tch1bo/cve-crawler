#!/usr/bin/python

import config
import db
import logging
import misc
import models
import net
import progressbar
import re
import time
import urlparse

github_commit_re = re.compile(r"https?://github.com/.+/commits?/([0-9a-f]+)")
github_commit_relative_re = re.compile(r'href="/.+/commits?/([0-9a-f]+)"')
github_issue_re = re.compile(r"https?://github.com/\S+/issues/.+")
github_pull_re = re.compile(r"https?://github.com/\S+/pull/.+")
svn_apache_re = re.compile(r"https?://svn.apache.org/.+")
svn_apache_revision_re = re.compile(".+/r(\d+)$")
svn_apache_message_re = re.compile(r"<pre class=\"vc_log\">(.+)</pre>", re.DOTALL)
commit_hash_re = re.compile(r"[0-9a-f]{,40}$", re.IGNORECASE)


def _get_queries_for_hash_url(url_string, hashes):
    hashes = misc.unique(hashes)
    if config.IGNORE_SHORT_HASHES:
        hashes = [h for h in hashes if len(h) == 40]
    queries = \
        [db.InsertQuery(models.CommitHash, hash=h) for h in hashes] +\
        [db.ConnectQuery(models.hash_url_table, h, url_string) for h in hashes]
    return queries

def _get_queries_for_search_url(url_string, search_query):
    search_query = search_query[:config.MAX_GITHUB_QUERY_LEN]
    queries = [db.InsertQuery(models.GithubSearchQuery, query=search_query),
               db.ConnectQuery(models.query_url_table, search_query, url_string)]
    return queries

def extract_from_github_commit(url_string):
    queries = []
    match = re.match(github_commit_re, url_string)
    if match:
        h = match.group(1)
        if len(h) < 5 or len(h) > 40:
            # Fix for urls like https://github.com/curl/curl/commit/curl-7_50_2~32
            print "got weird hash: '{0} for url: {1}".format(h, url_string)
        else:
            queries = _get_queries_for_hash_url(url_string, [h])
    return queries

def extract_from_github_issue(url_string):
    queries = []
    if re.match(github_issue_re, url_string):
        try:
            raw = net.get_raw_resource(url_string, auth=None)
        except:
            return []
        hashes = re.findall(github_commit_re, raw)
        hashes += re.findall(github_commit_relative_re, raw)
        queries = _get_queries_for_hash_url(url_string, hashes)
    return queries

def extract_from_github_pull(url_string):
    queries = []
    if re.match(github_pull_re, url_string):
        commit_match = re.match(r".+/commits?/([0-9a-f]+)/?$", url_string)
        if commit_match:
            return _get_queries_for_hash_url(url_string, [commit_match.group(1)])
        normalized_url = re.sub(r"/?files.+$", "", url_string)
        try:
            raw = net.get_raw_resource(normalized_url + "/commits/", auth=None)
        except:
            return []
        hashes = re.findall(github_commit_re, raw)
        hashes += re.findall(github_commit_relative_re, raw)
        queries = _get_queries_for_hash_url(url_string, hashes)
    return queries

def extract_from_apache_svn(url_string):
    # TODO: search other svn urls.
    if not re.match(svn_apache_re, url_string):
        return []
    db_queries = []
    query_t = "org:apache {0}"
    params = urlparse.parse_qs(urlparse.urlparse(url_string).query)

    # Search by revision id.
    r1 = r2 = revision_id = None
    for k, v in params.iteritems():
        if k in ["rev", "revision"]:
            revision_id = v[0]
        elif k == "r1":
            r1 = v[0]
        elif k == "r2":
            r2 = v[0]
    if not revision_id:
        if r1 and r2:
            revision_id = max(r1, r2)
        else:
            match = re.match(svn_apache_revision_re, url_string)
            if match:
                revision_id = match.group(1)
    if revision_id:
        query_string = query_t.format(revision_id)
        db_queries += _get_queries_for_search_url(url_string, query_string)

    # Search by commit message.
    try:
        raw = net.get_raw_resource(url_string)
        messages = re.findall(svn_apache_message_re, raw)
        if messages:
            query_string = query_t.format(messages[0])
            db_queries += _get_queries_for_search_url(url_string, query_string)
    except Exception as e:
        print "got exception for: {0}".format(url_string)

    return db_queries

def extract_from_commit_urls(url_string):
    if "commit" not in url_string:
        return []

    queries = []
    parse = urlparse.urlparse(url_string)
    params = urlparse.parse_qs(parse.query)
    h = params.get('id') or params.get('h') or params.get('commit')
    if h and re.match(commit_hash_re, h[0]):
        queries += _get_queries_for_hash_url(url_string, [h[0]])

    path_fragments = parse.path.split("/")
    index = misc.find_in_list(path_fragments, "commit", None) or \
            misc.find_in_list(path_fragments, "commits", None) or \
            misc.find_in_list(path_fragments, "commitdiff", None)
    if index and index < len(path_fragments) - 1 and \
            re.match(commit_hash_re, path_fragments[index+1]):
        queries += _get_queries_for_hash_url(url_string, [path_fragments[index+1]])
    return queries

def extract_from_googlesource(url_string):
    if "googlesource" not in url_string:
        return []
    queries = []
    path_fragments = urlparse.urlparse(url_string).path.split("/")
    index = misc.find_in_list(path_fragments, "+", None)
    if index and index < len(path_fragments) - 1 and \
            re.match(commit_hash_re, path_fragments[index+1]):
        queries += _get_queries_for_hash_url(url_string, [path_fragments[index+1]])
    return queries

def extract_from_moodle(url_string):
    if "git.moodle.org" not in url_string or "MDL" not in url_string:
        # Links without "MDL" and with "commit" are processed in
        # extract_from_commit_urls.
        return []
    try:
        raw = net.get_raw_resource(url_string)
    except:
        return []
    hashes = re.findall(r"<a href=.+?h=([0-9a-f]{40})\">commit</a>", raw)
    return _get_queries_for_hash_url(url_string, hashes)

def extract_from_chromium_codereview(url_string):
    if "codereview.chromium.org" not in url_string:
        return []
    try:
        raw = net.get_raw_resource(url_string)
    except:
        print "got exception for: " + url_string
        return []
    hashes = re.findall(r"Committed: <a href=\S+/([0-9a-f]{40})\">", raw)
    return _get_queries_for_hash_url(url_string, hashes)


def dump_commits(parallel=True):
    black_list_res = [
        # This list is used to temporarily disable some of the urls to not waste
        # time on processing them.

        # re.compile(r".+git.kernel.org.+"),
        # re.compile(r".+github.com.+"),
        # re.compile(r".+svn.apache.org.+"),
        # re.compile(".+github.+(linux).+"),
    ]
    def black_list(url_string):
        for black_list_re in black_list_res:
            if re.match(black_list_re, url_string):
                return True
        return False

    def extract(url_string):
        extractors = [
            extract_from_github_commit,
            extract_from_github_issue,
            extract_from_github_pull,
            extract_from_apache_svn,
            extract_from_commit_urls,
            extract_from_googlesource,
            extract_from_moodle,
            extract_from_chromium_codereview,
            # TODO: extract from git kernel org
        ]
        queries = []
        for e in extractors:
            queries = e(url_string)
            if queries:
                break
        bar.update()
        return queries

    print "Parsing URLs"
    url_strings = [x.url
            for x in db.global_session.query(models.URL).all()
            if not black_list(x.url) and not x.hashes and not x.queries]
    bar = misc.KnownLengthBar(maxval=len(url_strings), parallel=parallel)
    queries = misc.map_func(extract, url_strings, parallel)
    queries = misc.unique(misc.flatten_list(queries))
    print "Parsing URLs done"

    if not queries:
        print "No new hashes :("
    else:
        print "Storing results"
        db.process_queries(queries)
        print "Storing results done"

    print "Writing bad urls to {0}".format(config.BAD_URLS_FILE)
    good_urls_set = set([q.right_unique_value for q in queries
        if q.__class__ == db.ConnectQuery and
        q.table in [models.hash_url_table, models.query_url_table]])
    bad_urls = [x for x in url_strings if x not in good_urls_set]

    with open(config.BAD_URLS_FILE, "w") as f:
        f.write("\n".join(sorted(bad_urls)))

def _messages_match(a, b):
    a_set = set(a.split())
    b_set = set(b.split())
    common_count = len(a_set.intersection(b_set))
    return common_count > len(a_set) / 2 and common_count > len(b_set) / 2

def _do_github_search_query(search_query):
    logger = logging.getLogger("github_search")
    def remove_hrefs(s):
        s = re.sub(r"<a href.+?>", "", s)
        s = s.replace("</a>", "")
        return s

    mutations = [
            lambda x: [x],
            lambda x: [remove_hrefs(x)],
            lambda x: remove_hrefs(x).split("\n")
    ]
    mutants = misc.unique(misc.flatten_list([m(search_query.query) for m in mutations]))
    for query_str in mutants:
        if not query_str:
            continue
        logger.info("trying {0}".format(query_str))
        code, answer = net.github_search(query_str)
        if code == net.CODE_TIMEOUT:
            logger.info("sleeping...")
            mutants.append(query_str) # Try again.
            time.sleep(60)
        elif code == net.CODE_VALIDATION:
            logger.info("got 422: " + answer)
            search_query.state = models.GithubSearchQuery.ERROR
            db.global_session.commit()
        elif code == net.CODE_OK:
            if len(answer["items"]) > 5:
                answer["items"] = [item for item in answer["items"]
                    if _messages_match(search_query.query, item["commit"]["message"])]
            hash_strings = misc.unique([item["sha"] for item in answer["items"]])
            logger.info("got results: {0}".format(hash_strings))
            queries = []
            if hash_strings:
                search_query.state = models.GithubSearchQuery.NON_EMPTY
                for h in hash_strings:
                    queries += [
                        db.InsertQuery(models.CommitHash, hash=h),
                        db.ConnectQuery(models.query_hash_table, search_query.query, h)
                    ]
                    queries += [db.ConnectQuery(models.hash_url_table, h, url.url)
                        for url in search_query.urls]
                db.process_queries(queries)
                db.global_session.commit() # Commit state update.
                return
            search_query.state = models.GithubSearchQuery.EMPTY
            db.global_session.commit() # Commit state update.
        else:
            raise "got something unexpected: {0} {1}".format(code, answer)

def github_search_loop():
    logger = logging.getLogger("github_search")
    logger.setLevel(logging.INFO)
    fh = logging.FileHandler('github_search.log')
    fh.setLevel(logging.DEBUG)
    logger.addHandler(fh)

    queries = db.global_session.query(models.GithubSearchQuery).filter(
            models.GithubSearchQuery.state == models.GithubSearchQuery.NOT_SEEN).all()
    bar = misc.KnownLengthBar(maxval=len(queries), parallel=False)
    for i, q in bar(enumerate(queries)):
        logger.info("({0}/{1}) searching: '{2}'".format(i, len(queries), q.query))
        _do_github_search_query(q)
        logger.info("-" * 80)

