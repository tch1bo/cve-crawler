#!/usr/bin/python

import config
import db
import misc
import models
import net
import progressbar
import re
import urlparse

github_commit_re = re.compile(r"https?://github.com/.+/commit/([0-9a-f]+)")
github_commit_relative_re = re.compile(r'href="/.+/commits?/([0-9a-f]+)"')
github_issue_re = re.compile(r"https?://github.com/\S+/issues/.+")
github_pull_re = re.compile(r"https?://github.com/\S+/pull/.+")
svn_apache_re = re.compile(r"https?://svn.apache.org/.+")
svn_apache_revision_re = re.compile(".+/r(\d+)$")
svn_apache_message_re = re.compile(r"<pre class=\"vc_log\">(.+)</pre>", re.DOTALL)

# TODO: normalize hashes to 40-bytes strings.
# TODO: make the db work with multithreading.

def _get_queries_for_hash_url(url_string, hashes):
    hashes = misc.unique(hashes)
    queries = \
        [db.InsertQuery(models.CommitHash, hash=h) for h in hashes] +\
        [db.ConnectQuery(models.hash_url_table, h, url_string) for h in hashes]
    return queries

def _get_queries_for_search_url(url_string, search_query):
    search_query = search_query[:config.MAX_GITHUB_QUERY_LEN]
    queries = [db.InsertQuery(models.GithubSearchQuery, query=search_query),
               db.ConnectQuery(models.query_url_table, search_query, url_string)]
    return queries

def extract_from_github_commit(url_string):
    queries = []
    match = re.match(github_commit_re, url_string)
    if match:
        h = match.group(1)
        if len(h) < 5 or len(h) > 40:
            # Fix for urls like https://github.com/curl/curl/commit/curl-7_50_2~32
            print "got weird hash: '{0} for url: {1}".format(h, url_string)
        else:
            queries = _get_queries_for_hash_url(url_string, [h])
    return queries

def extract_from_github_issue(url_string):
    queries = []
    if re.match(github_issue_re, url_string):
        try:
            raw = net.get_raw_resource(url_string, auth=None)
        except:
            return []
        hashes = re.findall(github_commit_re, raw)
        hashes += re.findall(github_commit_relative_re, raw)
        queries = _get_queries_for_hash_url(url_string, hashes)
    return queries

def extract_from_github_pull(url_string):
    queries = []
    if re.match(github_pull_re, url_string):
        commit_match = re.match(r".+/commits/([0-9a-f]+)/?$", url_string)
        if commit_match:
            return _get_queries_for_hash_url(url_string, [commit_match.group(1)])
        normalized_url = re.sub(r"/?files.+$", "", url_string)
        try:
            raw = net.get_raw_resource(normalized_url + "/commits/", auth=None)
        except:
            return []
        hashes = re.findall(github_commit_re, raw)
        hashes += re.findall(github_commit_relative_re, raw)
        queries = _get_queries_for_hash_url(url_string, hashes)
    return queries

def extract_from_apache_svn(url_string):
    # TODO: After queries have been resolved, connect urls and found hashes.
    if not re.match(svn_apache_re, url_string):
        return []
    db_queries = []
    query_t = "org:apache {0}"
    params = urlparse.parse_qs(urlparse.urlparse(url_string).query)

    # Search by revision id.
    revision_id = None
    for k, v in params.iteritems():
        if k in ["rev", "revision"]:
            revision_id = v[0]
        break
    else:
        match = re.match(svn_apache_revision_re, url_string)
        if match:
            revision_id = match.group(1)
    if revision_id:
        query_string = query_t.format(revision_id)
        db_queries += _get_queries_for_search_url(url_string, query_string)

    # Search by commit message.
    try:
        raw = net.get_raw_resource(url_string)
        messages = re.findall(svn_apache_message_re, raw)
        if messages:
            query_string = query_t.format(messages[0])
            db_queries += _get_queries_for_search_url(url_string, query_string)
    except Exception as e:
        print "got exception for: {0}".format(url_string)

    return db_queries

def dump_commits(parallel=True):
    black_list_res = [
        # This list is used to temporarily disable some of the urls to not waste
        # time on processing them.

        re.compile(r".+git.kernel.org.+"),
        # re.compile(r".+github.com.+"),
        # re.compile(r".+svn.apache.org.+"),
        # re.compile(".+github.+(linux).+"),
    ]
    def black_list(url_string):
        for black_list_re in black_list_res:
            if re.match(black_list_re, url_string):
                return True
        return False

    def extract(url_string):
        extractors = [
            extract_from_github_commit,
            extract_from_github_issue,
            extract_from_github_pull,
            extract_from_apache_svn,
            # extract from git kernel org
        ]
        queries = []
        for e in extractors:
            queries = e(url_string)
            if queries:
                break
        bar.update()
        return queries

    print "Parsing URLs"
    url_strings = [x.url
            for x in db.global_session.query(models.URL).all()
            if not black_list(x.url) and not x.hashes and not x.queries]
    bar = misc.KnownLengthBar(maxval=len(url_strings), parallel=parallel)
    queries = misc.map_func(extract, url_strings, parallel)
    queries = misc.unique(misc.flatten_list(queries))
    print "Parsing URLs done"

    if not queries:
        print "No new hashes :("
    else:
        print "Storing results"
        db.process_queries(queries)
        print "Storing results done"

    print "Writing bad urls to {0}".format(config.BAD_URLS_FILE)
    good_urls_set = set([q.right_unique_value for q in queries
        if q.__class__ == db.ConnectQuery and q.table == models.hash_url_table])
    bad_urls = [x for x in url_strings if x not in good_urls_set]

    with open(config.BAD_URLS_FILE, "w") as f:
        f.write("\n".join(sorted(bad_urls)))
