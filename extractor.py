#!/usr/bin/python

import config
import db
import misc
import models
import net
import progressbar
import re
import urlparse

github_commit_re = re.compile(r"https?://github.com/.+/commit/([0-9a-f]+)")
github_commit_relative_re = re.compile(r'href="/.+/commits?/([0-9a-f]+)"')
github_issue_re = re.compile(r"https?://github.com/\S+/issues/.+")
github_pull_re = re.compile(r"https?://github.com/\S+/pull/.+")
svn_apache_re = re.compile(r"https?://svn.apache.org/.+")
svn_apache_revision_re = re.compile(".+/r(\d+)$")

# TODO: normalize hashes to 40-bytes strings.
# TODO: make the db work with multithreading.

# Each extractor should return a tuple: (url, [hashes]).
def extract_from_github_commit(url):
    match = re.match(github_commit_re, url.url)
    if not match:
        return (url, [])
    h = match.group(1)
    if len(h) < 5:
        # Fix for urls like https://github.com/curl/curl/commit/curl-7_50_2~32
        print "got weird hash: '{0} for url: {1}".format(h, url)
        return (url, [])
    return (url, [h])

def extract_from_github_issue(url):
    if not re.match(github_issue_re, url.url):
        return (url, [])
    try:
        raw = net.get_raw_resource(url.url, auth=None)
    except:
        return (url, [])
    hashes = re.findall(github_commit_re, raw)
    hashes += re.findall(github_commit_relative_re, raw)
    return (url, list(set(hashes)))

def extract_from_github_pull(url):
    if not re.match(github_pull_re, url.url):
        return (url, [])
    commit_match = re.match(r".+/commits/([0-9a-f]+)/?$", url.url)
    if commit_match:
        return (url, [commit_match.group(1)])
    normalized_url = re.sub(r"/?files.+$", "", url.url)
    try:
        raw = net.get_raw_resource(normalized_url + "/commits/", auth=None)
    except:
        return (url, [])
    hashes = re.findall(github_commit_re, raw)
    hashes += re.findall(github_commit_relative_re, raw)
    return (url, list(set(hashes)))

def extract_from_apache_svn(url):
    # This one should run in single threaded mode only.
    if not re.match(svn_apache_re, url.url):
        return (url, [])
    params = urlparse.parse_qs(urlparse.urlparse(url.url).query)

    revision_id = None
    for k, v in params.iteritems():
        if k in ["rev", "revision"]:
            revision_id = v[0]
        break
    else:
        match = re.match(svn_apache_revision_re, url.url)
        if match:
            revision_id = match.group(1)
    if not revision_id:
        return (url, [])

    query = "org:apache {0}".format(revision_id)
    # If answer is empty also search by commit message.
    answer = net.github_search(query)
    if not answer:
        return (url, [])

    hashes = [i["sha"] for i in answer["items"]]
    return (url, hashes)

def dump_commits():
    dump_commits.black_list_res = [
        re.compile(r".+git.kernel.org.+"),
        re.compile(r".+github.com.+"), # REMOVE ME
        # re.compile(r".+svn.apache.org.+"),
        # re.compile(".+github.+(linux).+"),
    ]
    def black_list(url):
        return reduce(lambda x,y: x or re.match(y, url.url), dump_commits.black_list_res, False)

    def extract(url):
        extractors = [
            extract_from_github_commit,
            extract_from_github_issue,
            extract_from_github_pull,
            extract_from_apache_svn,
            # extract from git kernel org
        ]
        result = reduce(lambda x,y: x or y(url)[1], extractors, None)
        dump_commits.bar.update()
        return (url, result)

    print "Parsing URLs"
    urls = db.session.query(models.URL).all()

    # Remove black listed urls.
    urls = filter(lambda x: not black_list(x), urls)

    # Remove urls which already have associated hashes.
    urls = filter(lambda x: not x.hashes, urls)
    dump_commits.bar = misc.KnownLengthBar(len(urls))

    # Extract and remove urls with no hashes.
    # extracted = misc.parallel_map(extract, urls)
    extracted = map(extract, urls)
    filtered = filter(lambda x: x[1], extracted)
    print "Parsing URLs done"
    if not filtered:
        # print extracted
        print "No new hashes :("
        return

    print "Storing results"
    hash_queries = []
    hashes_to_urls = []
    for e in filtered:
        for h in e[1]:
            hash_queries.append((("hash", h), ))
            hashes_to_urls.append((e[0], h))
    hashes = db.find_or_add_objects(models.CommitHash, hash_queries, True)
    bar = progressbar.ProgressBar(maxval=len(hashes_to_urls))

    hashes_to_urls_ids = []
    for x in bar(hashes_to_urls):
        h = hashes[(("hash", x[1]), )]
        hashes_to_urls_ids.append((("hash_id", h.id), ("url_id", x[0].id)))
    db.find_or_add_for_mtm_table(models.hash_url_table, hashes_to_urls_ids)

    print "Storing results done"

    bad = filter(lambda x: not x[1], extracted)
    with open(config.BAD_URLS_FILE, "w") as f:
        f.write("\n".join(sorted([x[0].url for x in bad])))
