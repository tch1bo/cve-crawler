#!/usr/bin/python

import config
import db
import misc
import models
import net
import progressbar
import re

github_commit_re = re.compile(r"https?://github.com/.+/commit/([0-9a-f]+)")
github_commit_relative_re = re.compile(r'href="/.+/commits?/([0-9a-f]+)"')
github_issue_re = re.compile(r"https?://github.com/\S+/issues/.+")
github_pull_re = re.compile(r"https?://github.com/\S+/pull/.+")

# Each extractor should return a tuple: (url, [hashes]).
def extract_from_github_commit(url):
    regex = github_commit_re
    match = re.match(regex, url.url)
    return (url, [match.group(1)]) if match else (url, [])

def extract_from_github_issue(url):
    if not re.match(github_issue_re, url.url):
        return (url, [])
    try:
        raw = net.get_raw_resource(url.url, auth=None)
    except:
        return (url, [])
    hashes = re.findall(github_commit_re, raw)
    hashes += re.findall(github_commit_relative_re, raw)
    return (url, list(set(hashes)))

def extract_from_github_pull(url):
    if not re.match(github_pull_re, url.url):
        return (url, [])
    commit_match = re.match(r".+/commits/([0-9a-f]+)/?$", url.url)
    if commit_match:
        return (url, [commit_match.group(1)])
    normalized_url = re.sub(r"/?files.+$", "", url.url)
    try:
        raw = net.get_raw_resource(normalized_url + "/commits/", auth=None)
    except:
        return (url, [])
    hashes = re.findall(github_commit_re, raw)
    hashes += re.findall(github_commit_relative_re, raw)
    return (url, list(set(hashes)))

def dump_commits():
    dump_commits.black_list_res = [
        re.compile(r".+git.kernel.org.+"),
        re.compile(r".+svn.apache.org.+"),
        # re.compile(".+github.+(linux).+"),
    ]
    def black_list(url):
        return reduce(lambda x,y: x or re.match(y, url.url), dump_commits.black_list_res, False)

    def extract(url):
        extractors = [
            extract_from_github_commit,
            extract_from_github_issue,
            extract_from_github_pull,
            # TODO: SVN apache
        ]
        result = reduce(lambda x,y: x or y(url)[1], extractors, None)
        dump_commits.bar.update()
        return (url, result)

    print "Parsing URLs"
    urls = db.session.query(models.URL).all()

    # Remove black listed urls.
    urls = filter(lambda x: not black_list(x), urls)

    # Remove urls which already have associated hashes.
    urls = filter(lambda x: not x.hashes, urls)
    dump_commits.bar = misc.KnownLengthBar(len(urls))

    # Extract and remove urls with no hashes.
    extracted = misc.parallel_map(extract, urls)
    filtered = filter(lambda x: x[1], extracted)
    print "Parsing URLs done"
    if not filtered:
        # print extracted
        print "No new hashes :("
        return

    print "Storing results"
    hash_queries = []
    hashes_to_urls = []
    for e in filtered:
        for h in e[1]:
            hash_queries.append((("hash", h), ))
            hashes_to_urls.append((e[0], h))
    hashes = db.find_or_add_objects(models.CommitHash, hash_queries, True)
    bar = progressbar.ProgressBar(maxval=len(hashes_to_urls))

    hashes_to_urls_ids = []
    for x in bar(hashes_to_urls):
        h = hashes[(("hash", x[1]), )]
        hashes_to_urls_ids.append((("hash_id", h.id), ("url_id", x[0].id)))
    db.find_or_add_for_mtm_table(models.hash_url_table, hashes_to_urls_ids)

    print "Storing results done"

    bad = filter(lambda x: not x[1], extracted)
    with open(config.BAD_URLS_FILE, "w") as f:
        f.write("\n".join([x[0].url for x in bad]))
