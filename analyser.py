import db
import misc
import models
import re

from os.path import commonprefix
from urlparse import urlparse, urlunparse

def normalize_url(url):
    parse_result = urlparse(url)
    parse_result_list = list(parse_result)
    parse_result_list[0] = "http"
    parse_result_list[1] = re.sub(r":\d+", "", parse_result.netloc)
    parse_result_list[1] = re.sub(r"www\.", "", parse_result_list[1])
    return parse_result_list[1], urlunparse(parse_result_list)

class UrlCluster:
    def __init__(self, netloc):
        self.netloc = netloc
        self.longest_prefix = netloc
        self.urls = set()

    def add_url(self, url):
        self.urls.add(url)

    def __len__(self):
        return len(self.urls)

    def __repr__(self):
        s = "Cluster: prefix='{0}', count='{1}', example='{2}'".\
                format(self.longest_prefix, len(self), list(self.urls)[0])
        return s

    def update_longest_prefix(self):
        self.longest_prefix = commonprefix(list(self.urls))
        return self.longest_prefix

class UrlClusterManager:
    def __init__(self):
        self.clusters = {}

    def add_url(self, url):
        netloc, new_url = normalize_url(url)
        self.clusters.setdefault(netloc, UrlCluster(netloc))
        self.clusters[netloc].add_url(new_url)

    def add_urls(self, urls):
        for url in urls:
            self.add_url(url)

    def str_n_clusters(self, start, finish):
        total_urls = sum([len(x) for x in self.clusters.values()])
        s = "ClusterManager: cluster_count={0} url_count={1}, clusters:\n\t".\
                format(len(self), total_urls)
        clusters = sorted(list(self.clusters.values()), key=len, reverse=True)[start:finish]
        return s + "\n\t".join([str(c) for c in clusters])

    def update_prefixes(self):
        for cluster in self.clusters.values():
            cluster.update_longest_prefix()

    def __len__(self):
        return len(self.clusters)

    def __repr__(self):
        return self.str_n_clusters(len(self))

def cluster_nist_urls():
    manager = UrlClusterManager()
    def items_handler(cve_items):
        get_urls = lambda item: item["cve"]["references"]["reference_data"]
        urls = [x["url"] for item in cve_items for x in get_urls(item)]
        manager.add_urls(urls)

    misc.crawl_nist_files(items_handler, parallel=False)
    manager.update_prefixes()
    return manager

def check_hashes_for_urls_like(template):
    urls = db.global_session.query(models.URL).all()
    urls = [url for url in urls if template in url.url]
    all_hashes = misc.unique([hash.hash for url in urls for hash in url.hashes])

    with open("./data/found_hashes.txt") as f:
        found_hashes = f.read().split("\n")[:-1]

    good = []
    bad = []
    for h in all_hashes:
        matches = filter(lambda x: x.startswith(h), found_hashes)
        if matches:
            # We want the longer hash.
            good.append(matches[0])
        else:
            bad.append(h)

    print "\n".join(good)
