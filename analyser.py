import config
import db
import json
import misc
import models
import net
import re
import time

from os.path import commonprefix
from urlparse import urlparse, urlunparse

def normalize_url(url):
    parse_result = urlparse(url)
    parse_result_list = list(parse_result)
    parse_result_list[0] = "http"
    parse_result_list[1] = re.sub(r":\d+", "", parse_result.netloc)
    parse_result_list[1] = re.sub(r"www\.", "", parse_result_list[1])
    return parse_result_list[1], urlunparse(parse_result_list)

class UrlCluster:
    def __init__(self, netloc):
        self.netloc = netloc
        self.longest_prefix = netloc
        self.urls = set()

    def add_url(self, url):
        self.urls.add(url)

    def __len__(self):
        return len(self.urls)

    def __repr__(self):
        s = "Cluster: prefix='{0}', count='{1}', example='{2}'".\
                format(self.longest_prefix, len(self), list(self.urls)[0])
        return s

    def update_longest_prefix(self):
        self.longest_prefix = commonprefix(list(self.urls))
        return self.longest_prefix

class UrlClusterManager:
    def __init__(self):
        self.clusters = {}

    def add_url(self, url):
        netloc, new_url = normalize_url(url)
        self.clusters.setdefault(netloc, UrlCluster(netloc))
        self.clusters[netloc].add_url(new_url)

    def add_urls(self, urls):
        for url in urls:
            self.add_url(url)

    def str_n_clusters(self, start, finish):
        total_urls = sum([len(x) for x in self.clusters.values()])
        s = "ClusterManager: cluster_count={0} url_count={1}, clusters:\n\t".\
                format(len(self), total_urls)
        clusters = sorted(list(self.clusters.values()), key=len, reverse=True)[start:finish]
        return s + "\n\t".join([str(c) for c in clusters])

    def update_prefixes(self):
        for cluster in self.clusters.values():
            cluster.update_longest_prefix()

    def __len__(self):
        return len(self.clusters)

    def __repr__(self):
        return self.str_n_clusters(len(self))

def cluster_nist_urls():
    manager = UrlClusterManager()
    def items_handler(cve_items):
        get_urls = lambda item: item["cve"]["references"]["reference_data"]
        urls = [x["url"] for item in cve_items for x in get_urls(item)]
        manager.add_urls(urls)

    misc.crawl_nist_files(items_handler, parallel=False)
    manager.update_prefixes()
    return manager

def get_found_hashes():
    with open("./data/found_hashes.txt") as f:
        return f.read().split("\n")[:-1]

def get_not_found_hashes():
    found_hashes = set(get_found_hashes())
    all_hashes = set([hash.hash for hash in
        db.global_session.query(models.CommitHash).all()])
    return list(all_hashes.difference(found_hashes))

def get_cached_commits():
    with open(config.HASH_CACHE_FILE, "r") as f:
        cached = json.load(f)
    return cached

def get_long_cached_commits():
    cached = get_cached_commits()
    return {k:v for k, v in cached.iteritems() if len(k) == 40}

def search_not_found_hashes():
    not_found = get_not_found_hashes()
    try:
        cached = get_cached_commits()
    except:
        cached = {}

    not_found = [x for x in not_found if x and cached.get(x) is None]
    if not not_found:
        print "No hashes to search!"
        return
    bar = misc.KnownLengthBar(maxval=len(not_found), parallel=False)
    start_time = bar.start_time
    for i, h in enumerate(not_found):
        bar.update(1)
        try:
            code, reply = net.github_search(h)
        except Exception as e:
            print "Got exception: {0} for hash: {1}".format(e, h)
            not_found.append(h)
            continue
        if code == net.CODE_OK:
            cached[h] = misc.unique([x["repository"]["full_name"] for x in reply["items"]])
        elif code == net.CODE_TIMEOUT:
            not_found.append(h)
            with open(config.HASH_CACHE_FILE, "w") as f:
                json.dump(cached, f, indent=2)
            # bar.finish()
            time.sleep(60)
            bar = misc.KnownLengthBar(maxval=len(not_found), parallel=False)
            bar.start_time = start_time
            bar.update(i)
        else:
            print "Got code {0} for hash: {1}".format(code, h)

    with open(config.HASH_CACHE_FILE, "w") as f:
        json.dump(cached, f, indent=2)

def cluster_not_found_hashes():
    cached = get_long_cached_commits()
    inverse_map = {}
    for commit in cached.keys():
        for repo in cached[commit]:
            inverse_map.setdefault(repo, set()).add(commit)

    def black_list(repo):
        black_listed_words = ["kernel", "linux", "moodle", "php"]
        repo = repo.lower()
        for word in black_listed_words:
            if word in repo:
                return True
        return False

    initial_len = len(inverse_map)
    inverse_map = {k:v for k, v in inverse_map.iteritems() if not black_list(k)}
    final_len = len(inverse_map)
    print "was: {0} got: {1}".format(initial_len, final_len)

    counts = [(k, len(v)) for k, v in inverse_map.iteritems()]
    with open("/tmp/1.txt", "w") as f:
        json.dump(sorted(counts, key=lambda x: x[1], reverse=True), f, indent=2)
    return inverse_map


def check_hashes_for_urls_like(template):
    urls = db.global_session.query(models.URL).all()
    urls = [url for url in urls if template in url.url]
    all_hashes = misc.unique([hash.hash for url in urls for hash in url.hashes])

    good = []
    bad = []
    for h in all_hashes:
        matches = filter(lambda x: x.startswith(h), get_found_hashes())
        if matches:
            # We want the longer hash.
            good.append(matches[0])
        else:
            bad.append(h)

    print "\n".join(good)
